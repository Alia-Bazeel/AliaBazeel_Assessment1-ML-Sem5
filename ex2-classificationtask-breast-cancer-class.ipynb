{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":408,"sourceType":"datasetVersion","datasetId":180},{"sourceId":9293783,"sourceType":"datasetVersion","datasetId":5626665}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Classification Task: *Breast Cancer Classification***","metadata":{}},{"cell_type":"markdown","source":"#### This project focuses on building a machine learning classifier to distinguish between benign and malignant breast masses. I have utilized the classic ***Breast Cancer Wisconsin (Diagnostic) Data Set*** to perform a binary classification, a critical task for aiding in early and accurate cancer diagnosis.","metadata":{}},{"cell_type":"markdown","source":"## **Step 1: Import Libraries**\n#### *The basic libraries that are required for data analysis and machine learning.*","metadata":{}},{"cell_type":"code","source":"import pandas as pd              # for handling data\nimport numpy as np               # for numerical operations\nimport matplotlib.pyplot as plt  # for data visualization\nimport seaborn as sns\n#import sklearn                   # scikit-learn library (machine learning tools)\nfrom sklearn.model_selection import train_test_split # split dataset into training and testing sets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression  # machine learning algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score,\n                             f1_score, confusion_matrix, classification_report,\n                             roc_curve, roc_auc_score, precision_recall_curve,\n                             ConfusionMatrixDisplay, auc)\n\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set(style='whitegrid')       # Set style for better visualization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.056274Z","iopub.execute_input":"2025-11-19T22:28:56.056678Z","iopub.status.idle":"2025-11-19T22:28:56.065571Z","shell.execute_reply.started":"2025-11-19T22:28:56.056653Z","shell.execute_reply":"2025-11-19T22:28:56.064423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 2: Load and Check Data**\n#### *Load the dataset and look at the first few rows to understand the structure.*","metadata":{}},{"cell_type":"code","source":"classify = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.067901Z","iopub.execute_input":"2025-11-19T22:28:56.068284Z","iopub.status.idle":"2025-11-19T22:28:56.086248Z","shell.execute_reply.started":"2025-11-19T22:28:56.068258Z","shell.execute_reply":"2025-11-19T22:28:56.084971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop duplicate id column if present\nif 'id' in classify.columns:\n    classify = classify.drop('id', axis=1)\nif 'Unnamed: 32' in classify.columns:\n    classify = classify.drop(columns=['Unnamed: 32'], errors='ignore')\n\nprint(\"\\nTarget value counts:\")\ndisplay(classify['diagnosis'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.087153Z","iopub.execute_input":"2025-11-19T22:28:56.087455Z","iopub.status.idle":"2025-11-19T22:28:56.098576Z","shell.execute_reply.started":"2025-11-19T22:28:56.087431Z","shell.execute_reply":"2025-11-19T22:28:56.097291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classify.info() #complete information  of the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.100017Z","iopub.execute_input":"2025-11-19T22:28:56.100380Z","iopub.status.idle":"2025-11-19T22:28:56.112628Z","shell.execute_reply.started":"2025-11-19T22:28:56.100353Z","shell.execute_reply":"2025-11-19T22:28:56.111352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classify.head() #first 5 rows of the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.115658Z","iopub.execute_input":"2025-11-19T22:28:56.115978Z","iopub.status.idle":"2025-11-19T22:28:56.142418Z","shell.execute_reply.started":"2025-11-19T22:28:56.115955Z","shell.execute_reply":"2025-11-19T22:28:56.141071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classify.tail() #last 5 rows of the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.143407Z","iopub.execute_input":"2025-11-19T22:28:56.143978Z","iopub.status.idle":"2025-11-19T22:28:56.166220Z","shell.execute_reply.started":"2025-11-19T22:28:56.143954Z","shell.execute_reply":"2025-11-19T22:28:56.165120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classify.shape #number of rows & coloumns of the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.167370Z","iopub.execute_input":"2025-11-19T22:28:56.167789Z","iopub.status.idle":"2025-11-19T22:28:56.174355Z","shell.execute_reply.started":"2025-11-19T22:28:56.167755Z","shell.execute_reply":"2025-11-19T22:28:56.173394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classify.describe() #statistical summary of numerical columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.175647Z","iopub.execute_input":"2025-11-19T22:28:56.175950Z","iopub.status.idle":"2025-11-19T22:28:56.245696Z","shell.execute_reply.started":"2025-11-19T22:28:56.175918Z","shell.execute_reply":"2025-11-19T22:28:56.244571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 3: Split the data into Training and Testing Sets**\n#### *We separate the features (X) and the target (y).*\n#### *- Encode target (M=malignant, B=benign)*  \n#### *- Check missing values*\n#### *- Create feature matrix X and target y*","metadata":{}},{"cell_type":"code","source":"# Encode diagnosis to 0/1: B=0, M=1\nclassify['target'] = classify['diagnosis'].map({'B':0, 'M':1})\n\n# Drop original diagnosis column\nclassify = classify.drop('diagnosis', axis=1)\n\n# Check missing\nprint(\"Missing values per column:\\n\", classify.isnull().sum().sort_values(ascending=False).head())\n\n# Prepare X,y\nX = classify.drop('target', axis=1)\ny = classify['target']\n\nprint(\"Features:\", X.shape[1])\nprint(\"Rows:\", X.shape[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.246832Z","iopub.execute_input":"2025-11-19T22:28:56.247127Z","iopub.status.idle":"2025-11-19T22:28:56.260578Z","shell.execute_reply.started":"2025-11-19T22:28:56.247103Z","shell.execute_reply":"2025-11-19T22:28:56.259319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Split into 80% training and 20% testing data**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.261392Z","iopub.execute_input":"2025-11-19T22:28:56.261673Z","iopub.status.idle":"2025-11-19T22:28:56.271890Z","shell.execute_reply.started":"2025-11-19T22:28:56.261653Z","shell.execute_reply":"2025-11-19T22:28:56.270548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\nprint(\"Train class distribution:\\n\", y_train.value_counts(normalize=True))\nprint(\"Test class distribution:\\n\", y_test.value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.272957Z","iopub.execute_input":"2025-11-19T22:28:56.273204Z","iopub.status.idle":"2025-11-19T22:28:56.281541Z","shell.execute_reply.started":"2025-11-19T22:28:56.273186Z","shell.execute_reply":"2025-11-19T22:28:56.280586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.282790Z","iopub.execute_input":"2025-11-19T22:28:56.283107Z","iopub.status.idle":"2025-11-19T22:28:56.311283Z","shell.execute_reply.started":"2025-11-19T22:28:56.283084Z","shell.execute_reply":"2025-11-19T22:28:56.310378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.312272Z","iopub.execute_input":"2025-11-19T22:28:56.312494Z","iopub.status.idle":"2025-11-19T22:28:56.320379Z","shell.execute_reply.started":"2025-11-19T22:28:56.312477Z","shell.execute_reply":"2025-11-19T22:28:56.319293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 4: Train - Model 1: *Logistic Regression***\n#### *We create the model and fit it (train it) on the training data.*","metadata":{}},{"cell_type":"code","source":"model_log = LogisticRegression(max_iter=1000)  # max_iter=1000 ensures the model converges\nmodel_log.fit(X_train.fillna(0), y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:28:56.323745Z","iopub.execute_input":"2025-11-19T22:28:56.324012Z","iopub.status.idle":"2025-11-19T22:29:01.776999Z","shell.execute_reply.started":"2025-11-19T22:28:56.323989Z","shell.execute_reply":"2025-11-19T22:29:01.774747Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 5: Make Predictions - Model 1: *Logistic Regression***\n#### *Use the trained model to predict on the test set.*","metadata":{}},{"cell_type":"code","source":"y_pred_log = model_log.predict(X_test.fillna(0))\n\ny_prob_log = model_log.predict_proba(X_test.fillna(0))[:, 1]  # for ROC/AUC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:01.777623Z","iopub.execute_input":"2025-11-19T22:29:01.777865Z","iopub.status.idle":"2025-11-19T22:29:01.791183Z","shell.execute_reply.started":"2025-11-19T22:29:01.777847Z","shell.execute_reply":"2025-11-19T22:29:01.790408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 6: Evaluate Model - Model 1: *Logistic Regression*** \n#### *Accuracy shows how many predictions were correct.*\n#### *The confusion matrix shows how many true/false predictions were made.*","metadata":{}},{"cell_type":"code","source":"accuracy_log = accuracy_score(y_test, y_pred_log)\ncm_log = confusion_matrix(y_test, y_pred_log)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:01.792225Z","iopub.execute_input":"2025-11-19T22:29:01.792489Z","iopub.status.idle":"2025-11-19T22:29:01.802713Z","shell.execute_reply.started":"2025-11-19T22:29:01.792467Z","shell.execute_reply":"2025-11-19T22:29:01.801960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Accuracy of the model:\", round(accuracy_log, 3))\nprint(\"\\nConfusion Matrix:\")\nprint(cm_log)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:01.805777Z","iopub.execute_input":"2025-11-19T22:29:01.806771Z","iopub.status.idle":"2025-11-19T22:29:01.815530Z","shell.execute_reply.started":"2025-11-19T22:29:01.806740Z","shell.execute_reply":"2025-11-19T22:29:01.814756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 7: Display Classification Report - Model 1: *Logistic Regression***\n#### *Evaluate the Logistic Regression model performance with precision, recall, f1-score for each class.*","metadata":{}},{"cell_type":"code","source":"print(\"\\nClassification Report:\")    # Printing detailed evaluation metrics\nprint(classification_report(y_test, y_pred_log))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:01.816144Z","iopub.execute_input":"2025-11-19T22:29:01.816378Z","iopub.status.idle":"2025-11-19T22:29:01.845534Z","shell.execute_reply.started":"2025-11-19T22:29:01.816357Z","shell.execute_reply":"2025-11-19T22:29:01.844794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 8: Visualize - Model 1: *Logistic Regression***","metadata":{}},{"cell_type":"markdown","source":"## -------- 8.1. *Confusion Matrix*\n#### *Heatmap helps understand prediction correctness visually.*","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.heatmap(cm_log, annot=True, fmt='d')\nplt.title(\"Confusion Matrix Heatmap\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Actual Values\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:01.849547Z","iopub.execute_input":"2025-11-19T22:29:01.849882Z","iopub.status.idle":"2025-11-19T22:29:02.089212Z","shell.execute_reply.started":"2025-11-19T22:29:01.849859Z","shell.execute_reply":"2025-11-19T22:29:02.088242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## -------- 8.2. *ROC Curve*\n#### *Visualize trade-off between True Positive Rate and False Positive Rate*","metadata":{}},{"cell_type":"code","source":"y_prob_log = model_log.predict_proba(X_test.fillna(0))[:,1]  # probabilities for positive class\nfpr, tpr, thresholds = roc_curve(y_test, y_prob_log)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, color='darkorange', label=f'ROC Curve (AUC = {roc_auc:.2f})')\nplt.plot([0,1],[0,1], color='navy', linestyle='--')\nplt.title('ROC Curve - Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:02.090151Z","iopub.execute_input":"2025-11-19T22:29:02.090405Z","iopub.status.idle":"2025-11-19T22:29:02.353781Z","shell.execute_reply.started":"2025-11-19T22:29:02.090378Z","shell.execute_reply":"2025-11-19T22:29:02.352611Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## -------- 8.3. *Precision-Recall Curve*\n#### *Visualize trade-off between Precision and Recall*","metadata":{}},{"cell_type":"code","source":"precision, recall, thresholds = precision_recall_curve(y_test, y_prob_log)\n\nplt.figure(figsize=(6,4))\nplt.plot(recall, precision, color='green')\nplt.title('Precision-Recall Curve - Logistic Regression')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:02.354601Z","iopub.execute_input":"2025-11-19T22:29:02.354864Z","iopub.status.idle":"2025-11-19T22:29:02.597559Z","shell.execute_reply.started":"2025-11-19T22:29:02.354843Z","shell.execute_reply":"2025-11-19T22:29:02.596631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 9: Compare Training vs Testing Score - Model 1: *Logistic Regression***\n#### *Check how well the Logistic Regression model generalizes on unseen data.*\n#### *(Check if the model is overfitting or underfitting.)*","metadata":{}},{"cell_type":"code","source":"train_score = model_log.score(X_train.fillna(0), y_train)\ntest_score = model_log.score(X_test.fillna(0), y_test)\n\nprint(\"Training Accuracy:\", round(train_score, 3))\nprint(\"Testing Accuracy:\", round(test_score, 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:02.598900Z","iopub.execute_input":"2025-11-19T22:29:02.599283Z","iopub.status.idle":"2025-11-19T22:29:02.613322Z","shell.execute_reply.started":"2025-11-19T22:29:02.599253Z","shell.execute_reply":"2025-11-19T22:29:02.612049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 10: Train - Model 2: *Random Forest Classifier***\n#### *Random Forest does not strictly require scaling; we fit and compute probabilities for ROC.*","metadata":{}},{"cell_type":"code","source":"# Initialize the Random Forest Classifier\nmodel_ran = RandomForestClassifier(n_estimators=200, random_state=42)\n\n# Train the model\nmodel_ran.fit(X_train.fillna(0), y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:02.614619Z","iopub.execute_input":"2025-11-19T22:29:02.614968Z","iopub.status.idle":"2025-11-19T22:29:03.093135Z","shell.execute_reply.started":"2025-11-19T22:29:02.614934Z","shell.execute_reply":"2025-11-19T22:29:03.091747Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 11: Make Predictions - Model 2: *Random Forest Classifier***","metadata":{}},{"cell_type":"code","source":"y_pred_ran = model_ran.predict(X_test.fillna(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:03.094178Z","iopub.execute_input":"2025-11-19T22:29:03.094429Z","iopub.status.idle":"2025-11-19T22:29:03.115900Z","shell.execute_reply.started":"2025-11-19T22:29:03.094411Z","shell.execute_reply":"2025-11-19T22:29:03.114436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 12: Evaluate Model - Model 2: *Random Forest Classifier*** ","metadata":{}},{"cell_type":"code","source":"# Calculate accuracy\naccuracy_ran = accuracy_score(y_test, y_pred_ran)\n\n# Generate confusion matrix\ncm_ran = confusion_matrix(y_test, y_pred_ran)\n\n# Print evaluation metrics\nprint(\"Random Forest Model Accuracy:\", round(accuracy_ran, 3))\nprint(\"\\nConfusion Matrix:\")\nprint(cm_ran)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_ran))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:03.116845Z","iopub.execute_input":"2025-11-19T22:29:03.117100Z","iopub.status.idle":"2025-11-19T22:29:03.138981Z","shell.execute_reply.started":"2025-11-19T22:29:03.117080Z","shell.execute_reply":"2025-11-19T22:29:03.137797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 13: Display Classification Report - Model 2: *Random Forest Classifier***\n#### *Evaluate the Random Forest model performance with precision, recall, f1-score for each class.*","metadata":{}},{"cell_type":"code","source":"print(\"\\nClassification Report:\")    # Printing detailed evaluation metrics\nprint(classification_report(y_test, y_pred_ran))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:03.140082Z","iopub.execute_input":"2025-11-19T22:29:03.140345Z","iopub.status.idle":"2025-11-19T22:29:03.155274Z","shell.execute_reply.started":"2025-11-19T22:29:03.140323Z","shell.execute_reply":"2025-11-19T22:29:03.154341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 14: Visualize - Model 2: *Random Forest Classifier***","metadata":{}},{"cell_type":"markdown","source":"## -------- 14.1. *Confusion Matrix*\n#### *Heatmap helps understand prediction correctness visually.*","metadata":{}},{"cell_type":"code","source":"# confusion matrix\nplt.figure(figsize=(6,4))\nsns.heatmap(cm_ran, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\nplt.title(\"Random Forest Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:03.156299Z","iopub.execute_input":"2025-11-19T22:29:03.156730Z","iopub.status.idle":"2025-11-19T22:29:03.403171Z","shell.execute_reply.started":"2025-11-19T22:29:03.156695Z","shell.execute_reply":"2025-11-19T22:29:03.402105Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## -------- 14.2. *ROC Curve*\n#### *Visualize trade-off between True Positive Rate and False Positive Rate*","metadata":{}},{"cell_type":"code","source":"y_prob_ran = model_ran.predict_proba(X_test.fillna(0))[:,1]  # probabilities for positive class\nfpr, tpr, thresholds = roc_curve(y_test, y_prob_ran)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, color='darkorange', label=f'ROC Curve (AUC = {roc_auc:.2f})')\nplt.plot([0,1],[0,1], color='navy', linestyle='--')\nplt.title('ROC Curve - Random Forest Classifier')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:03.404204Z","iopub.execute_input":"2025-11-19T22:29:03.404435Z","iopub.status.idle":"2025-11-19T22:29:03.701349Z","shell.execute_reply.started":"2025-11-19T22:29:03.404417Z","shell.execute_reply":"2025-11-19T22:29:03.700046Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## -------- 14.3. *Precision-Recall Curve*\n#### *Visualize trade-off between Precision and Recall*","metadata":{}},{"cell_type":"code","source":"precision, recall, thresholds = precision_recall_curve(y_test, y_prob_ran)\n\nplt.figure(figsize=(6,4))\nplt.plot(recall, precision, color='green')\nplt.title('Precision-Recall Curve - Random Forest Classifier')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:03.702367Z","iopub.execute_input":"2025-11-19T22:29:03.702651Z","iopub.status.idle":"2025-11-19T22:29:04.136907Z","shell.execute_reply.started":"2025-11-19T22:29:03.702629Z","shell.execute_reply":"2025-11-19T22:29:04.135839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 15: Compare Training vs Testing Score - Model 2: *Random Forest Classifier***\n#### *Check how well the Random Forest model generalizes on unseen data.*\n#### *(Check if the model is overfitting or underfitting.)*","metadata":{}},{"cell_type":"code","source":"train_score = model_ran.score(X_train.fillna(0), y_train)\ntest_score = model_ran.score(X_test.fillna(0), y_test)\n\nprint(\"Training Accuracy:\", round(train_score, 3))\nprint(\"Testing Accuracy:\", round(test_score, 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:04.138064Z","iopub.execute_input":"2025-11-19T22:29:04.138400Z","iopub.status.idle":"2025-11-19T22:29:04.178779Z","shell.execute_reply.started":"2025-11-19T22:29:04.138374Z","shell.execute_reply":"2025-11-19T22:29:04.177659Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 16: Compare Model Performance**","metadata":{}},{"cell_type":"markdown","source":"## -------- 16.1. *Accuracy*","metadata":{}},{"cell_type":"code","source":"# Compare accuracies\nprint(\"Logistic Regression Accuracy:\", round(accuracy_log, 3))\nprint(\"Random Forest Accuracy:\", round(accuracy_ran, 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:04.180967Z","iopub.execute_input":"2025-11-19T22:29:04.181296Z","iopub.status.idle":"2025-11-19T22:29:04.187235Z","shell.execute_reply.started":"2025-11-19T22:29:04.181273Z","shell.execute_reply":"2025-11-19T22:29:04.185624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## -------- 16.2. *Horizontal Bar Plot for Accuracy*","metadata":{}},{"cell_type":"code","source":"# Define models and their corresponding accuracies\nmodels = ['Logistic Regression', 'Random Forest']\naccuracies = [accuracy_log, accuracy_ran]\n\nplt.figure(figsize=(6,4))\n\n# Horizontal bar plot\nplt.barh(models, accuracies, color=['skyblue', 'lightgreen'])\nplt.xlim(0, 1)\nplt.title('Model Accuracy Comparison')\nplt.xlabel('Accuracy')\nfor i, acc in enumerate(accuracies):\n    plt.text(acc + 0.005, i, str(round(acc,3)), va='center')  # annotate bars with accuracy\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T22:29:04.188595Z","iopub.execute_input":"2025-11-19T22:29:04.188953Z","iopub.status.idle":"2025-11-19T22:29:04.412958Z","shell.execute_reply.started":"2025-11-19T22:29:04.188921Z","shell.execute_reply":"2025-11-19T22:29:04.411916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Summary**\n*300-500 words of all the code, including dataset description, preprocessing, model implementation, results, and interpretation*","metadata":{}},{"cell_type":"markdown","source":"In this classification task, my goal was to build machine learning models to classify breast tumors as either benign or malignant using the Wisconsin Diagnostic Breast Cancer (WDBC) dataset. The dataset included 569 samples, each with 30 numerical features that were extracted from digitized images of fine needle aspirates. These features described different characteristics of cell nuclei, and the target variable indicated whether the tumor was benign (0) or malignant (1), making the task suitable for a binary classification problem.\n\nI started by loading the dataset into a pandas DataFrame and reviewing its structure with functions such as .info(), .describe(), .head(), and .tail(). From this initial exploration, I confirmed that the dataset had no missing values and that all the features were numerical, which simplified the preprocessing stage. After that, I separated the features (X) from the target variable (y) and created an 80:20 training-testing split so I could evaluate how well the models would perform on unseen data.\n\nThe first model I implemented was Logistic Regression, which is a widely used method for binary classification. I trained the model on the training set and made predictions on the test set. To evaluate its performance, I looked at accuracy, the confusion matrix, and visualizations such as the ROC curve and Precision-Recall curve. These metrics and plots helped me understand how well the model balanced true positives and false positives, and the overall accuracy showed that Logistic Regression performed strongly on this dataset.\n\nTo compare results and potentially improve performance, I then used a Random Forest Classifier. This algorithm builds multiple decision trees and combines their outputs, which helps capture more complex patterns and reduce overfitting. After training the Random Forest model, I evaluated it using accuracy, the classification report, the confusion matrix, and a comparison of training and testing scores. The Random Forest Classifier slightly outperformed Logistic Regression, showing better predictive strength and good generalization.\n\nI also used visualizations such as heatmaps, ROC curves, and Precision-Recall curves to better interpret the results. These plots made it easier to compare both models and understand the strengths of each one. Logistic Regression offered simplicity and interpretability, while the Random Forest model proved more robust and accurate overall.\n\nThis task helped me practice the full pipeline for a classification problem: exploring the dataset, preparing the features, choosing suitable models, training them, evaluating the results, and using visualizations to interpret performance. Both models worked well, but the Random Forest classifier showed slightly better predictive ability. Overall, this exercise strengthened my understanding of supervised classification and highlighted the importance of using multiple evaluation metrics when working with real-world datasets.","metadata":{}}]}